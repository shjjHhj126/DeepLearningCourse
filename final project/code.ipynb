{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LDvK7QBTFm8v"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUhenTOiFNvF",
        "outputId": "49ca97ca-ce63-46f4-b7e1-2e5e185c8081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/DeepLearningCourse/final project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moOiCrQMFf31",
        "outputId": "29a15af9-df77-4465-a8a6-a319d8c0e422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearningCourse/final project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import argparse\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import unicodedata\n",
        "import json\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "CgEDh1ywFsPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH4JmdqqOxsH",
        "outputId": "3be70341-135d-45ee-f8f4-63c0256a20fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "artMOcBdta80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    common = collections.Counter(a_gold) & collections.Counter(a_pred)\n",
        "    num_same = sum(common.values())\n",
        "    if len(a_gold) == 0 or len(a_pred) == 0:\n",
        "      # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "      return int(a_gold == a_pred)\n",
        "    if num_same == 0:\n",
        "      return 0\n",
        "    precision = 1.0 * num_same / len(a_pred)\n",
        "    recall = 1.0 * num_same / len(a_gold)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(a_gold == a_pred)"
      ],
      "metadata": {
        "id": "7kdCC5sml6Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###data preprocessing\n"
      ],
      "metadata": {
        "id": "LDvK7QBTFm8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean data, save to json"
      ],
      "metadata": {
        "id": "ictEAJLmhS91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd3pA_hxPcs4",
        "outputId": "86277498-351e-4156-cbe0-f244d199a8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_from_json(filename):\n",
        "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
        "    with open(filename) as data_file:\n",
        "        data = json.load(data_file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "4D_eIhEaFpC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_exs(dataset):\n",
        "    \"\"\"\n",
        "    Returns the total number of (context, question, answer) triples,\n",
        "    given the data read from the SQuAD json file.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    for article in dataset['data']:\n",
        "        for para in article['paragraphs']:\n",
        "            total += len(para['qas'])\n",
        "    return total"
      ],
      "metadata": {
        "id": "6xzil5U6G4Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(sequence):\n",
        "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
        "    return tokens\n",
        "\n",
        "def get_char_word_loc_mapping(context, context_tokens):\n",
        "    \"\"\"\n",
        "    Return a mapping that maps from character locations to the corresponding token locations.\n",
        "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
        "    Inputs:\n",
        "      context: string (unicode)\n",
        "      context_tokens: list of strings (unicode)\n",
        "    Returns:\n",
        "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
        "        Only ints corresponding to non-space character locations are in the keys\n",
        "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
        "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
        "    \"\"\"\n",
        "    acc = '' # accumulator\n",
        "    current_token_idx = 0 # current word loc\n",
        "    mapping = dict()\n",
        "\n",
        "    for char_idx, char in enumerate(context): # step through original characters\n",
        "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
        "            acc += char # add to accumulator\n",
        "            context_token = unicodedata.normalize('NFKD', context_tokens[current_token_idx]) # current word token\n",
        "            if acc == context_token: # if the accumulator now matches the current word token\n",
        "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
        "                for char_loc in range(syn_start, char_idx+1):\n",
        "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
        "                acc = '' # reset accumulator\n",
        "                current_token_idx += 1\n",
        "\n",
        "    if current_token_idx != len(context_tokens):\n",
        "        return None\n",
        "    else:\n",
        "        return mapping\n",
        "\n",
        "def preprocess_and_write(dataset, tier, out_dir):\n",
        "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them\n",
        "    and calculates answer span in terms of token indices.\n",
        "    Note: due to tokenization issues, and the fact that the original answer\n",
        "    spans are given in terms of characters, some examples are discarded because\n",
        "    we cannot get a clean span in terms of tokens.\n",
        "    This function produces the {train/dev}_{context/question/answer/span} files.\n",
        "    Inputs:\n",
        "      dataset: read from JSON\n",
        "      tier: string (\"train\" or \"dev\")\n",
        "      out_dir: directory to write the preprocessed files\n",
        "    Returns:\n",
        "      the number of (context, question, answer) triples written to file by the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    num_exs = 0 # number of examples written to file\n",
        "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
        "    examples = []\n",
        "\n",
        "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
        "\n",
        "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
        "        for pid in range(len(article_paragraphs)):\n",
        "\n",
        "            context = unicodedata.normalize('NFKD', article_paragraphs[pid]['context']) # string\n",
        "\n",
        "            # The following replacements are suggested in the paper\n",
        "            # BidAF (Seo et al., 2016)\n",
        "            context = context.replace(\"''\", '\" ')\n",
        "            context = context.replace(\"``\", '\" ')\n",
        "\n",
        "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
        "            context = context.lower()\n",
        "\n",
        "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
        "\n",
        "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
        "\n",
        "            if charloc2wordloc is None: # there was a problem\n",
        "                num_mappingprob += len(qas)\n",
        "                continue # skip this context example\n",
        "\n",
        "            # for each question, process the question and answer and write to file\n",
        "            for qn in qas:\n",
        "\n",
        "                # read the question text and tokenize\n",
        "                question = unicodedata.normalize('NFKD', qn['question']) # string\n",
        "                question_tokens = tokenize(question) # list of strings\n",
        "\n",
        "                # of the three answers, just take the first\n",
        "                if len(qn['answers'])!=0:\n",
        "                    ans_text = unicodedata.normalize('NFKD', qn['answers'][0]['text']).lower() # get the answer text\n",
        "                    ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
        "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
        "\n",
        "                    # Check that the provided character spans match the provided answer text\n",
        "                    if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
        "                      # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
        "                      # We should upgrade to Python 3 next year!\n",
        "                      num_spanalignprob += 1\n",
        "                      continue\n",
        "\n",
        "                    # get word locs for answer start and end (inclusive)\n",
        "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
        "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
        "                    assert ans_start_wordloc <= ans_end_wordloc\n",
        "\n",
        "                    # Check retrieved answer tokens match the provided answer text.\n",
        "                    # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
        "                    # and the answer character span is around \"generation\",\n",
        "                    # but the tokenizer regards \"fifth-generation\" as a single token.\n",
        "                    # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
        "                    ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
        "                    if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
        "                        num_tokenprob += 1\n",
        "                        continue # skip this question/answer pair\n",
        "                else:\n",
        "                    ans_text = 'not in the paragraph'\n",
        "                    ans_start_charloc = -1\n",
        "                    ans_end_charloc = -1\n",
        "\n",
        "                # examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
        "                examples.append((context,question,ans_text,ans_start_charloc,ans_end_charloc))\n",
        "                # print(ans_text,ans_start_charloc,ans_end_charloc)\n",
        "\n",
        "                num_exs += 1\n",
        "\n",
        "    print(\"\\nNumber of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
        "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
        "\n",
        "    with open(out_dir+\"/\"+tier+\".json\", 'w', encoding='utf-8') as json_file:\n",
        "        # shuffle examples\n",
        "        random.shuffle(examples)\n",
        "\n",
        "        dictionary = {\"data\": examples}\n",
        "        # Convert dictionary to JSON\n",
        "        json_data = json.dumps(dictionary)\n",
        "        json_file.write(json_data)\n"
      ],
      "metadata": {
        "id": "w2lr4mTTI0PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filename = \"train-v2.0.json\"\n",
        "dev_filename = \"dev-v2.0.json\"\n",
        "\n",
        "train_data = data_from_json(train_filename)\n",
        "dev_data = data_from_json(dev_filename)\n",
        "print( \"Train data has %i examples total\" % total_exs(train_data))\n",
        "print( \"Dev data has %i examples total\" % total_exs(dev_data))\n",
        "preprocess_and_write(train_data, 'train', \"preprocessed_data\")\n",
        "preprocess_and_write(dev_data, 'val', \"preprocessed_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRmypfYGSVX",
        "outputId": "89889d1a-0bbf-4d78-e8e9-51914653fda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data has 130319 examples total\n",
            "Dev data has 11873 examples total\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train: 100%|██████████| 442/442 [00:34<00:00, 12.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1071\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  6089\n",
            "Processed 123159 examples of total 130319\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing val: 100%|██████████| 35/35 [00:02<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  67\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  346\n",
            "Processed 11460 examples of total 11873\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "HZHvsA8ctPCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, AdamW\n",
        "from transformers import ElectraForQuestionAnswering, ElectraTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "0KsOBmBAb6wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare model input data structure"
      ],
      "metadata": {
        "id": "sx33Ey9Osj7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = json.load(open('preprocessed_data/train.json'))\n",
        "val_data = json.load(open('preprocessed_data/val.json'))"
      ],
      "metadata": {
        "id": "lkHrlhDon3H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data['data'][:10000]\n",
        "val_data = val_data['data'][:1000]"
      ],
      "metadata": {
        "id": "VPuD9GdMSxZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_seq_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, question, answer, start_position, end_position = self.data[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_seq_length,\n",
        "            truncation=\"only_second\",  # Truncate context, not the question\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"start_positions\": start_position,\n",
        "            \"end_positions\": end_position\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "6KNl7LOsSjqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a,b,c,d,e in train_data[:10]:\n",
        "  print(c,d,e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNtI1HQAFYDs",
        "outputId": "393b92e8-a044-4dab-d8a3-b3a1c0c9fb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bourgeois stores 506 522\n",
            "not in the paragraph -1 -1\n",
            "not in the paragraph -1 -1\n",
            "not in the paragraph -1 -1\n",
            "radio silence was observed until the bombs fell 326 373\n",
            "understand the meaning of the words they are singing. 619 672\n",
            "jiangnan examination hall 361 386\n",
            "fourth 453 459\n",
            "not in the paragraph -1 -1\n",
            "not in the paragraph -1 -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### base models"
      ],
      "metadata": {
        "id": "SEMnnF9fASaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "model = ElectraForQuestionAnswering.from_pretrained(\"google/electra-small-discriminator\", output_hidden_states=True)\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZltwszgtS0K",
        "outputId": "c09484f6-9cc1-407b-b447-88178433812a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = CustomDataset(train_data, tokenizer, max_seq_length=384)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = CustomDataset(val_data, tokenizer, max_seq_length=384)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "jHB4CueuSkeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0izUzbqKS8l8",
        "outputId": "ee7ec540-feb3-45ee-8a68-9aee2269c18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForQuestionAnswering(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def compute_loss(self, start_logits, start_logits_pos, start_positions, end_logits, end_logits_pos, end_positions):\n",
        "        # print(start_logits, start_positions)\n",
        "        start_loss = self.helper_compute_loss(start_logits, start_logits_pos, start_positions)\n",
        "        end_loss = self.helper_compute_loss(end_logits, end_logits_pos, end_positions)\n",
        "        total_loss = (start_loss + end_loss) / 2.0\n",
        "        return total_loss\n",
        "\n",
        "    def helper_compute_loss(self, logits, logits_pos, positions):\n",
        "        mask = (positions != -1) | (logits >= 0.5)\n",
        "\n",
        "        non_valid_logits = logits[~mask]\n",
        "        non_valid_positions = torch.ones_like(positions[~mask])\n",
        "\n",
        "        valid_logits = logits[mask]\n",
        "        valid_positions = positions[mask]\n",
        "\n",
        "        loss = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "        loss_a = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "        loss_b = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "\n",
        "        if valid_logits.size(0) > 0:\n",
        "            # loss_a += self.loss_fct(valid_logits_pos, valid_positions)/ valid_positions.size(0)\n",
        "            loss_a += self.loss_fct(valid_logits, valid_positions)/ valid_positions.size(0)\n",
        "        if non_valid_logits.size(0) > 0:\n",
        "            # loss_b += self.loss_fct(non_valid_logits_pos, non_valid_positions)/ non_valid_positions.size(0)\n",
        "            loss_b += self.loss_fct(non_valid_logits, non_valid_positions)/ non_valid_positions.size(0)\n",
        "        loss += loss_a\n",
        "        loss += loss_b\n",
        "        return loss"
      ],
      "metadata": {
        "id": "j1_9XV4safd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_function = CustomLoss()"
      ],
      "metadata": {
        "id": "3AICtnWYZ7s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix0jMWXOmFER",
        "outputId": "5549d557-3da7-44d1-ddf0-672a96dc335f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9064498d90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "plot_f1 = []\n",
        "plot_em = []\n",
        "plot_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "    f1_scores = []\n",
        "    em_scores = []\n",
        "    losses = []\n",
        "    i=0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        start_positions = batch[\"start_positions\"].to(device)\n",
        "        end_positions = batch[\"end_positions\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # start_positions=start_positions,\n",
        "            # end_positions=end_positions\n",
        "        )\n",
        "        print(outputs.start_logits.shape)\n",
        "\n",
        "        start_logits, start_logits_pos = torch.max(outputs.start_logits, dim=1, keepdim=True)\n",
        "        end_logits, end_logits_pos = torch.max(outputs.end_logits, dim=1, keepdim=True)\n",
        "        start_logits_pos = start_logits_pos.float().to(device)\n",
        "        end_logits_pos = end_logits_pos.float().to(device)\n",
        "        start_logits_pos.requires_grad = True\n",
        "        end_logits_pos.requires_grad = True\n",
        "\n",
        "        loss = loss_function.compute_loss(\n",
        "            start_logits.squeeze().float(), start_logits_pos.squeeze().float(),\n",
        "            start_positions.float(), end_logits.squeeze().float(),\n",
        "            end_logits_pos.squeeze().float(), end_positions.float()\n",
        "            )\n",
        "        losses.extend([loss.item()])\n",
        "\n",
        "        context_ids = input_ids.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        # Calculate F1 score and EM\n",
        "        for (start, end), (pred_start_prob, pred_end_prob), (pred_start, pred_end), context in zip(zip(start_positions, end_positions), zip(start_logits, end_logits), zip(start_logits_pos, end_logits_pos), context_ids):\n",
        "            start, end, pred_start, pred_end = int(start), int(end), int(pred_start), int(pred_end)\n",
        "            # true_answer\n",
        "            if start == -1 and end == -1:\n",
        "                true_answer = \"not in the paragraph\"\n",
        "            else:\n",
        "                true_answer = tokenizer.decode(context[start:end+1])\n",
        "\n",
        "            # pred_answer\n",
        "            threshold = 0.5\n",
        "            if pred_start_prob < threshold or pred_end_prob < threshold:\n",
        "                pred_answer = \"not in the paragraph\"\n",
        "            else:\n",
        "                pred_answer = tokenizer.decode(context[pred_start:pred_end+1])\n",
        "            f1_scores.append(compute_f1(true_answer, pred_answer))\n",
        "            em_scores.append(compute_exact(true_answer, pred_answer))\n",
        "\n",
        "        f = (batch_size*i)/10000\n",
        "        if i%20==0:\n",
        "            print(f'finished {f*100:.2f}%, loss:{loss.item():.2f}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        i+=1\n",
        "    i=0\n",
        "\n",
        "    model.eval()\n",
        "    val_f1_scores = []\n",
        "    val_em_scores = []\n",
        "    val_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "            input_ids = val_batch[\"input_ids\"].to(device)\n",
        "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
        "            start_positions = val_batch[\"start_positions\"].to(device)\n",
        "            end_positions = val_batch[\"end_positions\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                start_positions=start_positions,\n",
        "                end_positions=end_positions\n",
        "            )\n",
        "\n",
        "            start_logits, start_logits_pos = torch.max(outputs.start_logits, dim=1, keepdim=True)\n",
        "            end_logits, end_logits_pos = torch.max(outputs.end_logits, dim=1, keepdim=True)\n",
        "\n",
        "            val_loss = loss_function.compute_loss(\n",
        "                start_logits.squeeze().float(), start_logits_pos.squeeze().float(),\n",
        "                start_positions.float(), end_logits.squeeze().float(),\n",
        "                end_logits_pos.squeeze().float(), end_positions.float()\n",
        "            )\n",
        "            val_losses.extend([val_loss.item()])\n",
        "\n",
        "            start_logits_pos = start_logits_pos.detach().cpu().numpy()\n",
        "            end_logits_pos = end_logits_pos.detach().cpu().numpy()\n",
        "            context_ids = input_ids.detach().cpu().numpy()\n",
        "            start_positions = start_positions.detach().cpu().numpy()\n",
        "            end_positions = end_positions.detach().cpu().numpy()\n",
        "\n",
        "            # Calculate F1 score and EM\n",
        "            for (start, end), (pred_start_prob, pred_end_prob), (pred_start, pred_end), context in zip(zip(start_positions, end_positions), zip(start_logits, end_logits), zip(start_logits_pos, end_logits_pos), context_ids):\n",
        "                start, end, pred_start, pred_end = int(start), int(end), int(pred_start), int(pred_end)\n",
        "                # true_answer\n",
        "                if start == -1 and end == -1:\n",
        "                    true_answer = \"not in the paragraph\"\n",
        "                else:\n",
        "                    true_answer = tokenizer.decode(context[start:end+1])\n",
        "\n",
        "                # pred_answer\n",
        "                threshold = 0.5\n",
        "                if pred_start_prob < threshold or pred_end_prob < threshold:\n",
        "                    pred_answer = \"not in the paragraph\"\n",
        "                else:\n",
        "                    pred_answer = tokenizer.decode(context[pred_start:pred_end+1])\n",
        "                val_f1_scores.append(compute_f1(true_answer, pred_answer))\n",
        "                val_em_scores.append(compute_exact(true_answer, pred_answer))\n",
        "\n",
        "\n",
        "    # Perform evaluation\n",
        "    avg_f1_score = np.mean(f1_scores)\n",
        "    avg_em_score = np.mean(em_scores)\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    avg_val_f1_score = np.mean(val_f1_scores)\n",
        "    avg_val_em_score = np.mean(val_em_scores)\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - F1 Score: {avg_f1_score:.4f}, EM Score: {avg_em_score:.4f}, Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Validation - F1 Score: {avg_val_f1_score:.4f}, EM Score: {avg_val_em_score:.4f}, Loss: {avg_val_loss:.4f}\")\n",
        "    plot_f1.append(avg_f1_score)\n",
        "    plot_em.append(avg_em_score)\n",
        "    plot_loss.append(avg_loss)\n",
        "\n",
        "    model.train()\n"
      ],
      "metadata": {
        "id": "SdrpWDHlS2dM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "27e75355-79c5-45bd-8859-8ee219bf8ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 384])\n",
            "finished 0.00%, loss:176.45\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d1ca664b2669>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             val_loss = loss_function.compute_loss(\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mstart_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_logits_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mstart_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-1b3d28446214>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, start_logits, start_logits_pos, start_positions, end_logits, end_logits_pos, end_positions)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_logits_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# print(start_logits, start_positions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mstart_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_logits_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mend_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-1b3d28446214>\u001b[0m in \u001b[0;36mhelper_compute_loss\u001b[0;34m(self, logits, logits_pos, positions)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnon_valid_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mnon_valid_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvalid_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"DistilAlbertModel\")\n",
        "# model.save_pretrained(\"ElectraModel\")"
      ],
      "metadata": {
        "id": "tEDm5W1yVzsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of trainable parameters: {num_params}\")"
      ],
      "metadata": {
        "id": "Dw9Rw4Vgf9mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### my model"
      ],
      "metadata": {
        "id": "TjRTe0a48M8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "DBknnCg-ajZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ElectraModel, ElectraConfig\n",
        "\n",
        "class ElectraBiDirectionalAttention(nn.Module):\n",
        "    def __init__(self, electra_model, hidden_size, num_labels, num_heads=8):\n",
        "        super(ElectraBiDirectionalAttention, self).__init__()\n",
        "        self.electra = electra_model\n",
        "        self.attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.attention_linear = nn.Linear(hidden_size, hidden_size * 2)\n",
        "        self.start_classifier = nn.Linear(hidden_size * 2, 1)\n",
        "        self.end_classifier = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, start_positions, end_positions):\n",
        "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        print(sequence_output.shape)  # torch.Size([2, 384, 768])\n",
        "\n",
        "        # Transpose sequence_output to have shape (seq_len, batch_size, hidden_size)\n",
        "        sequence_output = sequence_output.transpose(0, 1)\n",
        "\n",
        "        # Apply multihead attention\n",
        "        attention_output, attention_output_weights = self.attention(sequence_output, sequence_output, sequence_output)\n",
        "        attention_output = attention_output.transpose(0, 1)\n",
        "        print(attention_output.shape)  # torch.Size([2, 384, 768])\n",
        "\n",
        "        # Apply linear layer to reshape attention_output\n",
        "        attention_output = self.attention_linear(attention_output)\n",
        "\n",
        "        start_logits = self.start_classifier(attention_output).squeeze(-1)\n",
        "        end_logits = self.end_classifier(attention_output).squeeze(-1)\n",
        "\n",
        "        output = {\n",
        "            \"start_logits\": start_logits,\n",
        "            \"end_logits\": end_logits\n",
        "        }\n",
        "\n",
        "        return output\n",
        "\n",
        "num_heads = 8\n"
      ],
      "metadata": {
        "id": "7LvJ2xwcSlLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ElectraBiLSTMAttention(nn.Module):\n",
        "    def __init__(self, electra_model, hidden_size, num_labels):\n",
        "        super(ElectraBiLSTMAttention, self).__init__()\n",
        "        self.electra = electra_model\n",
        "        self.bilstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
        "        self.start_classifier = nn.Linear(hidden_size * 2, 384)\n",
        "        self.end_classifier = nn.Linear(hidden_size * 2, 384)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        lstm_output, _ = self.bilstm(sequence_output)\n",
        "\n",
        "        attention_scores = self.attention(lstm_output).squeeze(-1)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(-1)\n",
        "        attention_output = (lstm_output * attention_weights).sum(dim=1)\n",
        "\n",
        "        start_logits = self.start_classifier(attention_output)\n",
        "        end_logits = self.end_classifier(attention_output)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "metadata": {
        "id": "Cvr692rT7qui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraModel\n",
        "\n",
        "electra_model = ElectraModel.from_pretrained(\"google/electra-small-discriminator\", output_hidden_states = True)\n",
        "hidden_size = electra_model.config.hidden_size\n",
        "num_labels = 2  # start and end positions\n",
        "# model = ElectraBiDirectionalAttention(electra_model, hidden_size, num_labels)\n",
        "model = ElectraBiLSTMAttention(electra_model, hidden_size, num_labels)\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57VxUEJ2IcRn",
        "outputId": "5332d9ab-4b21-448d-dc21-f318b6a8717e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = CustomDataset(train_data, tokenizer, max_seq_length=384)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = CustomDataset(val_data, tokenizer, max_seq_length=384)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "4fFfF__qJqnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6C_X-UlpJfR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf1225d-c5fa-4834-d82d-550d9d796d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraBiLSTMAttention(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (bilstm): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
              "  (attention): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (start_classifier): Linear(in_features=512, out_features=384, bias=True)\n",
              "  (end_classifier): Linear(in_features=512, out_features=384, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def compute_loss(self, start_logits, start_positions, end_logits, end_positions):\n",
        "        # print(start_logits, start_positions)\n",
        "        start_loss = self.helper_compute_loss(start_logits, start_positions)\n",
        "        end_loss = self.helper_compute_loss(end_logits, end_positions)\n",
        "        total_loss = (start_loss + end_loss) / 2.0\n",
        "        return total_loss\n",
        "\n",
        "    def helper_compute_loss(self, logits, positions):\n",
        "        mask = (positions != -1) | (logits >= 0.5)\n",
        "\n",
        "        non_valid_logits = logits[~mask]\n",
        "        non_valid_positions = torch.ones_like(positions[~mask])\n",
        "\n",
        "        valid_logits = logits[mask]\n",
        "        valid_positions = positions[mask]\n",
        "\n",
        "        loss = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "        loss_a = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "        loss_b = torch.tensor(0.0, dtype=torch.float32, device=logits.device)\n",
        "\n",
        "        if valid_logits.size(0) > 0:\n",
        "            loss_a += self.loss_fct(valid_logits, valid_positions)/ valid_positions.size(0)\n",
        "        if non_valid_logits.size(0) > 0:\n",
        "            loss_b += self.loss_fct(non_valid_logits, non_valid_positions)/ non_valid_positions.size(0)\n",
        "        loss += loss_a\n",
        "        loss += loss_b\n",
        "        return loss"
      ],
      "metadata": {
        "id": "NVKlqvMk0Btt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_function = CustomLoss()"
      ],
      "metadata": {
        "id": "n99kiwXIJwNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_f1 = []\n",
        "plot_em = []\n",
        "plot_loss = []\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    f1_scores = []\n",
        "    em_scores = []\n",
        "    losses = []\n",
        "\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prepare batch data\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # every tensor involved in the backward propagation need to set requires_grad = True, some are set True automatically\n",
        "        start_positions = batch['start_positions'].float().to(device).requires_grad_(True)\n",
        "        end_positions = batch['end_positions'].float().to(device).requires_grad_(True)\n",
        "\n",
        "        # Forward pass\n",
        "        start_logits, end_logits = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # start_positions = start_positions,\n",
        "            # end_positions = end_positions\n",
        "        )\n",
        "\n",
        "        start_logits, start_logits_pos = torch.max(start_logits, dim=1, keepdim=True)\n",
        "        end_logits, end_logits_pos = torch.max(end_logits, dim=1, keepdim=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function.compute_loss(\n",
        "            start_logits.squeeze().float(),  start_positions.float(),\n",
        "            end_logits.squeeze().float(),  end_positions.float()\n",
        "            )\n",
        "        losses.extend([loss.item()])\n",
        "        context_ids = input_ids.detach().cpu().numpy()\n",
        "\n",
        "        start_logits = start_logits.detach().cpu().numpy()\n",
        "        end_logits = end_logits.detach().cpu().numpy()\n",
        "        start_positions = start_positions.detach().cpu().numpy()\n",
        "        end_positions = end_positions.detach().cpu().numpy()\n",
        "\n",
        "        # Calculate F1 score and EM\n",
        "        for (start, end), (pred_start_prob, pred_end_prob), (pred_start, pred_end), context in zip(zip(start_positions, end_positions), zip(start_logits, end_logits), zip(start_logits_pos, end_logits_pos), context_ids):\n",
        "            start, end, pred_start, pred_end = int(start), int(end), int(pred_start), int(pred_end)\n",
        "            # true_answer\n",
        "            if start == -1 and end == -1:\n",
        "                true_answer = \"not in the paragraph\"\n",
        "            else:\n",
        "                true_answer = tokenizer.decode(context[start:end+1])\n",
        "\n",
        "            # pred_answer\n",
        "            threshold = 0.5\n",
        "            if pred_start_prob < threshold or pred_end_prob < threshold:\n",
        "                pred_answer = \"not in the paragraph\"\n",
        "            else:\n",
        "                pred_answer = tokenizer.decode(context[pred_start:pred_end+1])\n",
        "            f1_scores.append(compute_f1(true_answer, pred_answer))\n",
        "            em_scores.append(compute_exact(true_answer, pred_answer))\n",
        "\n",
        "        f = (batch_size*i)/10000\n",
        "        if i%20==0:\n",
        "            print(f'finished {f*100:.2f}%, loss:{loss.item():.2f}')\n",
        "        i+=1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    i=0\n",
        "\n",
        "    model.eval()\n",
        "    val_f1_scores = []\n",
        "    val_em_scores = []\n",
        "    val_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "            input_ids = val_batch[\"input_ids\"].to(device)\n",
        "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
        "            start_positions = val_batch[\"start_positions\"].to(device)\n",
        "            end_positions = val_batch[\"end_positions\"].to(device)\n",
        "\n",
        "            start_logits, end_logits = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                # start_positions=start_positions,\n",
        "                # end_positions=end_positions\n",
        "            )\n",
        "            # logits = outputs.transpose(0, 1)\n",
        "            # start_logits = logits[0]\n",
        "            # end_logits = logits[1]\n",
        "\n",
        "            start_logits, start_logits_pos = torch.max(start_logits, dim=1, keepdim=True)\n",
        "            end_logits, end_logits_pos = torch.max(end_logits, dim=1, keepdim=True)\n",
        "\n",
        "            val_loss = loss_function.compute_loss(\n",
        "                start_logits.squeeze().float(),  start_positions.float(),\n",
        "                end_logits.squeeze().float(),  end_positions.float()\n",
        "            )\n",
        "            val_losses.extend([val_loss.item()])\n",
        "\n",
        "            start_logits_pos = start_logits_pos.detach().cpu().numpy()\n",
        "            end_logits_pos = end_logits_pos.detach().cpu().numpy()\n",
        "            context_ids = input_ids.detach().cpu().numpy()\n",
        "            start_positions = start_positions.detach().cpu().numpy()\n",
        "            end_positions = end_positions.detach().cpu().numpy()\n",
        "\n",
        "            # Calculate F1 score and EM\n",
        "            for (start, end), (pred_start_prob, pred_end_prob), (pred_start, pred_end), context in zip(zip(start_positions, end_positions), zip(start_logits, end_logits), zip(start_logits_pos, end_logits_pos), context_ids):\n",
        "                start, end, pred_start, pred_end = int(start), int(end), int(pred_start), int(pred_end)\n",
        "                # true_answer\n",
        "                if start == -1 and end == -1:\n",
        "                    true_answer = \"not in the paragraph\"\n",
        "                else:\n",
        "                    true_answer = tokenizer.decode(context[start:end+1])\n",
        "\n",
        "                # pred_answer\n",
        "                threshold = 0.5\n",
        "                if pred_start_prob < threshold or pred_end_prob < threshold:\n",
        "                    pred_answer = \"not in the paragraph\"\n",
        "                else:\n",
        "                    pred_answer = tokenizer.decode(context[pred_start:pred_end+1])\n",
        "                val_f1_scores.append(compute_f1(true_answer, pred_answer))\n",
        "                val_em_scores.append(compute_exact(true_answer, pred_answer))\n",
        "\n",
        "\n",
        "    # Perform evaluation\n",
        "    avg_f1_score = np.mean(f1_scores)\n",
        "    avg_em_score = np.mean(em_scores)\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    avg_val_f1_score = np.mean(val_f1_scores)\n",
        "    avg_val_em_score = np.mean(val_em_scores)\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - F1 Score: {avg_f1_score:.4f}, EM Score: {avg_em_score:.4f}, Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Validation - F1 Score: {avg_val_f1_score:.4f}, EM Score: {avg_val_em_score:.4f}, Loss: {avg_val_loss:.4f}\")\n",
        "    plot_f1.append(avg_f1_score)\n",
        "    plot_em.append(avg_em_score)\n",
        "    plot_loss.append(avg_loss)\n",
        "\n",
        "    model.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "QO5jz6GP8cXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}